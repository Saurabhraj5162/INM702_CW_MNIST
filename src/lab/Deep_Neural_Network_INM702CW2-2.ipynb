{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ6CMM4mU1IU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import truncnorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO4QzcBxv8Dn"
      },
      "outputs": [],
      "source": [
        "class My_Neural_Network():\n",
        "  def __init__(self, layers_list,\n",
        "               activation_functions,weight_initializer, \n",
        "               learning_rate,num_iterations, lambd,\n",
        "               drop_rate, sgd):\n",
        "\n",
        "    self.layers_list = layers_list\n",
        "    self.activation_functions = activation_functions\n",
        "    self.weight_initializer = weight_initializer\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iterations = num_iterations\n",
        "    self.lambd = lambd\n",
        "    self.drop_rate = drop_rate\n",
        "    self.sgd = sgd\n",
        "\n",
        "\n",
        "  def sigmoid(self,Z):\n",
        "    \"\"\" Arg: Linear activation Z\n",
        "        return: Sigmoid of Z\n",
        "    \"\"\"\n",
        "    return 1/(1+np.exp(-Z))\n",
        "\n",
        "  def sigmoid_derivative(self,dA,Z):\n",
        "    \"\"\" Arg: Derivative of Loss w.r.t activation (dL/dAl), Linear activation Z\n",
        "        return: dL/dZl = (dL/dAl)*(dAl/dZl)\n",
        "                (dAl/dZl) = Derivative of activation w.r.t linear activation(Z)\n",
        "    \"\"\"\n",
        "    dZl = self.sigmoid(Z)*(1-self.sigmoid(Z))\n",
        "\n",
        "    return dA*dZl\n",
        "\n",
        "  def relu(self,Z):\n",
        "    \"\"\" Arg: Linear activation Z\n",
        "        return: Relu of Z\n",
        "    \"\"\"\n",
        "    return np.maximum(0,Z)\n",
        "\n",
        "  def relu_derivative(self,dA,Z):\n",
        "    \"\"\" Arg: Derivative of Loss w.r.t activation (dL/dAl), Linear activation Z\n",
        "        return: dL/dZl = (dL/dAl)*(dAl/dZl)\n",
        "                (dAl/dZl) = Derivative of activation w.r.t linear activation(Z)\n",
        "    \"\"\"\n",
        "    \n",
        "    dZ = Z.copy()\n",
        "\n",
        "    dZ = np.array(dZ>0, dtype=np.float32)\n",
        "\n",
        "\n",
        "    return dA*dZ\n",
        "\n",
        "\n",
        "  def softmax(self,Z):\n",
        "    #print(Z.shape)\n",
        "    e = np.exp(Z)\n",
        "    e_total = np.sum(e, axis=0, keepdims=True)\n",
        "    #print(e.shape, e_total.shape)\n",
        "    \n",
        "    return np.divide(e,e_total)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def initialize_params(self, layers_list, method):\n",
        "    \"\"\" Args: layers_list : containing the number of neurons (nodes) for each layer in the netwrok.\n",
        "              method: a tuple. method[0] contains type of initialization (random/trunc_normal)\n",
        "                                method[1] contains multiplying_factor for random initialization\n",
        "                                          contains (mean, sd,low,upp) for truncated_normal\n",
        "        returns: Initialized parameters : Weights (W), Bias (b) for each layers in a dictionary data structure.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    params = {}\n",
        "    num_layers = len(layers_list)\n",
        "    if method == 'random':\n",
        "      mult_factor = 0.01\n",
        "      for l in range(1,num_layers):\n",
        "        params['W' + str(l)] = np.random.randn(layers_list[l], layers_list[l-1])*mult_factor\n",
        "        params['b' + str(l)] = np.zeros((layers_list[l], 1))\n",
        "\n",
        "    if method == 'trunc_normal':\n",
        "      mean, sd,low,upp = 0, 1, -0.5, 0.5\n",
        "      for l in range(1, num_layers):\n",
        "        temp = truncnorm((low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
        "        params['W' + str(l)] = temp.rvs((layers_list[l], layers_list[l-1]))\n",
        "        params['b' + str(l)] = np.zeros((layers_list[l], 1))\n",
        "\n",
        "    if method == 'zeros':\n",
        "      for l in range(1,num_layers):\n",
        "        params['W' + str(l)] = np.zeros((layers_list[l], layers_list[l-1]))\n",
        "        params['b' + str(l)] = np.zeros((layers_list[l], 1))\n",
        "\n",
        "\n",
        "    if method == 'he':\n",
        "      for l in range(1,num_layers):\n",
        "        params['W' + str(l)] = np.random.randn(layers_list[l], layers_list[l-1])*np.sqrt(2/layers_list[l-1])\n",
        "        params['b' + str(l)] = np.zeros((layers_list[l], 1))\n",
        "\n",
        "    if method == 'xavier':\n",
        "      for l in range(1,num_layers):\n",
        "        params['W' + str(l)] = np.random.randn(layers_list[l], layers_list[l-1])*np.sqrt(1/layers_list[l-1])\n",
        "        params['b' + str(l)] = np.zeros((layers_list[l], 1))\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "  def linear_activation(self, W,b,A,activation_function):\n",
        "    \"\"\"\n",
        "    Args: W,b - parameters for the current layer; \n",
        "          A- linear activation (Z = WA_prev + b) for the current layer;\n",
        "          activation_function\n",
        "    returns: A_forward, memories of current activation, dropout memory (if dropout)\n",
        "    \"\"\"\n",
        "\n",
        "    Z_current = np.dot(W,A) + b\n",
        "    #Z_current = np.squeeze(Z_current)\n",
        "    #print(Z_current.shape)\n",
        "    #print(Z_current)\n",
        "    linear_memory = (A,W,b)\n",
        "    drop_flag = 0\n",
        "    if activation_function == 'sigmoid':\n",
        "      A_current = self.sigmoid(Z_current)\n",
        "\n",
        "    if activation_function == 'relu':\n",
        "      A_current= self.relu(Z_current)\n",
        "\n",
        "    if activation_function == 'relu + dropout':\n",
        "      A_current= self.relu(Z_current)\n",
        "      D_current = np.random.rand(A_current.shape[0], A_current.shape[1])\n",
        "      D_current = (D_current < (1- self.drop_rate)).astype(int)\n",
        "      A_current = A_current*D_current\n",
        "      A_current = A_current/(1- self.drop_rate)\n",
        "      drop_flag = 1\n",
        "\n",
        "    \n",
        "    if activation_function == 'sigmoid + dropout':\n",
        "      A_current= self.sigmoid(Z_current)\n",
        "      D_current = np.random.rand(A_current.shape[0], A_current.shape[1])\n",
        "      D_current = (D_current < (1- self.drop_rate)).astype(int)\n",
        "      A_current = A_current*D_current\n",
        "      A_current = A_current/(1- self.drop_rate)\n",
        "      drop_flag = 1\n",
        "      \n",
        "\n",
        "    if activation_function == 'softmax':\n",
        "      A_current = self.softmax(Z_current)\n",
        "\n",
        "    if drop_flag == 0:\n",
        "      memory_current = (linear_memory, Z_current)\n",
        "    else:\n",
        "      memory_current = (linear_memory, Z_current, D_current)\n",
        "    return (A_current, memory_current)\n",
        "\n",
        "\n",
        "\n",
        "  def forward_propagation(self, A0, params, activation_functions):\n",
        "    \"\"\" \n",
        "    Args: A0 - X input\n",
        "          params - dictionary containing W and b for all the layers\n",
        "          activation_functions containing activation function used in all the layers (['relu',...,'sigmoid','softmax'])\n",
        "    returns: Activation output of last layer and the memory (W,b,Z,A) in each layer\n",
        "    \"\"\"\n",
        "    num_layers = len(params)//2\n",
        "    #print(f'shape of num_layers : {num_layers}')\n",
        "    memory = []\n",
        "    A_current = A0\n",
        "    for l in range(1,num_layers):\n",
        "      W_current = params['W' + str(l)]\n",
        "      b_current = params['b' + str(l)]\n",
        "      A_prev = A_current\n",
        "      #print(f'W : {W_current.shape} A : {A_current.shape}')\n",
        "      A_current, memory_current = self.linear_activation(W_current, b_current, A_prev, activation_functions[l])\n",
        "      memory.append(memory_current)\n",
        "    \n",
        "    W_current = params['W'+str(num_layers)]\n",
        "    b_current = params['b'+str(num_layers)]\n",
        "    #print(\"Activation for lasy layer : \", activation_functions[num_layers])\n",
        "    A_last, memory_current = self.linear_activation(W_current, b_current, A_current, activation_functions[num_layers])\n",
        "    memory.append(memory_current)  \n",
        "    #print('Output vs input shape of FWD prop:',A_last.shape, A0.shape)\n",
        "    #print(memory_current)\n",
        "    #print(memory)\n",
        "    return A_last, memory\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def calculate_cost(self, A_last, Y):\n",
        "    #print('cost : ', Y.shape)\n",
        "    m = Y.shape[1]\n",
        "    #print(Y.shape, A_last.shape)\n",
        "    part1 = Y*np.log(A_last)\n",
        "\n",
        "    cost = (-1/m)*np.sum(part1)\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost\n",
        "\n",
        "\n",
        "  def calculate_cost_L2_regularised(self, A_last, Y, parameters, lambd):\n",
        "    m = Y.shape[1]\n",
        "    L = len(parameters)//2\n",
        "    cost_without_reg = self.calculate_cost(A_last, Y)\n",
        "    L2_reg = 0\n",
        "    for l in range(1,L+1):\n",
        "      W = parameters['W' + str(l)]\n",
        "      L2_reg += np.sum(np.square(W))\n",
        "\n",
        "    L2_reg = (lambd/(2*m))*L2_reg\n",
        "\n",
        "    \n",
        "    cost = cost_without_reg + L2_reg\n",
        "    \n",
        "    return cost\n",
        "\n",
        "  def linear_backward(self,dA, memory, activation_function):\n",
        "    \"\"\" \n",
        "    Args: dA - derivative of next layer output w.r.t A of current layer\n",
        "          memory - contains values of Z,W,b,A for corresponding layer stored while forward propagation.\n",
        "          activation_function - 'relu'/'sigmoid'\n",
        "    returns: dA_previous (dL/dAl-1) - derivative of cost w.r.t activation of previous layers, \n",
        "            dW (dL/dWl)- derivative of cost w.r.t weight W of current layer\n",
        "            db (dL/db) -  derivative of cost w.r.t bias b of current layer\n",
        "    \"\"\"\n",
        "    \n",
        "    if self.drop_rate ==0:\n",
        "      A_previous, W, b = memory[0]\n",
        "      Z = memory[1]\n",
        "    else:\n",
        "      A_previous, W, b = memory[0]\n",
        "      Z = memory[1]\n",
        "      D = memory[2]\n",
        "\n",
        "    m = A_previous.shape[1]\n",
        "\n",
        "    if activation_function == 'relu':\n",
        "      dZ = self.relu_derivative(dA,Z)\n",
        "\n",
        "    elif activation_function == 'sigmoid':\n",
        "      dZ = self.sigmoid_derivative(dA,Z)\n",
        "\n",
        "    elif activation_function == 'relu + dropout':\n",
        "      dA = dA*D\n",
        "      dA = dA/(1-self.drop_rate)\n",
        "      dZ = self.relu_derivative(dA,Z)\n",
        "\n",
        "\n",
        "    elif activation_function == 'sigmoid + dropout':\n",
        "      dA = dA*D\n",
        "      dA = dA/(1-self.drop_rate)\n",
        "      dZ = self.sigmoid_derivative(dA,Z)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    dW = (1/m)*np.dot(dZ, A_previous.T)\n",
        "    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_previous = np.dot(W.T,dZ)\n",
        "\n",
        "    return dA_previous, dW, db\n",
        "\n",
        "\n",
        "  def backward_propagation(self,A_last, Y, memory, activation_functions):\n",
        "    \n",
        "    num_layers = len(memory)\n",
        "    grads = {}\n",
        "    Y = Y.reshape(A_last.shape)\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    #dA_last = A_last - Y\n",
        "\n",
        "    memory_current = memory[-1] #(A_L-1,W_L,b_L), Z_last(output of softmax layer)\n",
        "    dZ_last = A_last - Y\n",
        "\n",
        "    A_previous, W, b = memory_current[0]\n",
        "    \n",
        "    dW_current = (1/m)*np.dot(dZ_last, A_previous.T)\n",
        "    db_current = (1/m)*np.sum(dZ_last, axis=1, keepdims=True)\n",
        "    dA_previous = np.dot(W.T,dZ_last)\n",
        "\n",
        "    #dW_current, db_current = linear_backward(dZ_last, memory_current, activation_functions[-1])\n",
        "      \n",
        "    grads[\"dA\" + str(num_layers-1)] = dA_previous\n",
        "    grads[\"dW\" + str(num_layers)] = dW_current\n",
        "    grads[\"db\" + str(num_layers)] = db_current\n",
        "\n",
        "    for l in range(num_layers-2,-1,-1):\n",
        "      memory_current = memory[l]\n",
        "      #print('flkqenfnejnqn ', activation_functions[l])\n",
        "      dA_previous = grads[\"dA\" + str(l+1)]\n",
        "      dA_previous, dW_current, db_current = self.linear_backward(dA_previous, memory_current, activation_functions[l])\n",
        "      \n",
        "      grads[\"dA\" + str(l)] = dA_previous\n",
        "      grads[\"dW\" + str(l+1)] = dW_current\n",
        "      grads[\"db\" + str(l+1)] = db_current\n",
        "\n",
        "\n",
        "    return grads\n",
        "\n",
        "\n",
        "  def backward_propagation_L2_regularised(self, A_last, Y, memory, activation_functions, lambd):\n",
        "    \n",
        "    num_layers = len(memory)\n",
        "    grads = {}\n",
        "    Y = Y.reshape(A_last.shape)\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    memory_current = memory[-1] #(A_L-1,W_L,b_L), Z_last(output of softmax layer)\n",
        "    dZ_last = A_last - Y\n",
        "\n",
        "    A_previous, W, b = memory_current[0]\n",
        "    \n",
        "    dW_current = (1/m)*np.dot(dZ_last, A_previous.T) + (lambd/m)*W\n",
        "    db_current = (1/m)*np.sum(dZ_last, axis=1, keepdims=True)\n",
        "    dA_previous = np.dot(W.T,dZ_last)\n",
        "\n",
        "    #dW_current, db_current = linear_backward(dZ_last, memory_current, activation_functions[-1])\n",
        "      \n",
        "    grads[\"dA\" + str(num_layers-1)] = dA_previous\n",
        "    grads[\"dW\" + str(num_layers)] = dW_current\n",
        "    grads[\"db\" + str(num_layers)] = db_current\n",
        "\n",
        "    for l in range(num_layers-2,-1,-1):\n",
        "      memory_current = memory[l]\n",
        "      #print('flkqenfnejnqn ', activation_functions[l])\n",
        "      dA_previous = grads[\"dA\" + str(l+1)]\n",
        "      dA_previous, dW_current, db_current = self.linear_backward(dA_previous, memory_current, activation_functions[l])\n",
        "\n",
        "      A_previous_, W_, b_ = memory_current[0] #fetching memory cache\n",
        "      dW_current += (lambd/m)*W_    #adding L2 regularised term\n",
        "      \n",
        "      grads[\"dA\" + str(l)] = dA_previous\n",
        "      grads[\"dW\" + str(l+1)] = dW_current\n",
        "      grads[\"db\" + str(l+1)] = db_current\n",
        "\n",
        "\n",
        "    return grads\n",
        "\n",
        "  def gradient_descent(self,params, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Args: params - initialized parameters (W,b). {W1:__, b1:__}\n",
        "          grads - calculated gradients dictionary during backward_propagation\n",
        "          learning_rate - alpha of gradient descent\n",
        "\n",
        "    return : updated params dictionary\n",
        "    \"\"\"\n",
        "    num_layers = len(params)//2\n",
        "    updated_params = params.copy()\n",
        "    for l in range(1,num_layers+1):\n",
        "      updated_params['W' + str(l)] -= learning_rate*grads['dW' + str(l)]\n",
        "      updated_params['b' + str(l)] -= learning_rate*grads['db' + str(l)]\n",
        "\n",
        "    return updated_params\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def train(self,X,Y,print_cost=True):\n",
        "\n",
        "      # MY MODEL:\n",
        "      \"\"\"\n",
        "      def initialize_params(layers_list, method):\n",
        "          ...\n",
        "          return parameters \n",
        "      def forward_propagation(A0, params, activation_functions):\n",
        "          ...\n",
        "          return A_last, [((A_L-1,W_L,b_L), Z_last),\n",
        "                          ((A_L-2,W_L-1,b_L-1), Z_last-1),\n",
        "                          ((A_L-3,W_L-2,b_L-2), Z_last-2)\n",
        "                          .\n",
        "                          .\n",
        "                          .\n",
        "                          ((A0(X),W_1,b_1), Z_1)]\n",
        "      def calculate_cost(A_last, Y):\n",
        "          ...\n",
        "          return cost\n",
        "      def backward_propagation(A_last, Y, memory, activation_functions):\n",
        "          ...\n",
        "          return {dAL-1, dWL, dbL,\n",
        "                  dAL-2, dWL-1, dbL-1,\n",
        "                  .\n",
        "                  .\n",
        "                  .\n",
        "                  dA0, dW1, db1}\n",
        "      def gradient_descent(params, grads, learning_rate):\n",
        "          ...\n",
        "          return updated_parameters\n",
        "      \"\"\"\n",
        "\n",
        "\n",
        "      np.random.seed(1)\n",
        "      costs = []               \n",
        "      m = X.shape[1]\n",
        "      params = self.initialize_params(self.layers_list, self.weight_initializer)\n",
        "\n",
        "      #lopping for number of iterations:\n",
        "\n",
        "      if self.sgd == True:\n",
        "        for i in range(0, self.num_iterations):\n",
        "          total_cost = 0\n",
        "          for j in range(0,m):\n",
        "            \n",
        "            # Forward propagation:\n",
        "            print(np.reshape(X[:,j],(X.shape[0],1)))\n",
        "            A_last, memory = self.forward_propagation(np.reshape(X[:,j],(X.shape[0],1)), params, self.activation_functions)\n",
        "            #print('A Last : ',A_last.shape)\n",
        "            # Compute cost.\n",
        "\n",
        "            if self.lambd == 0:\n",
        "              total_cost += self.calculate_cost(A_last, np.reshape(Y[:,j],(Y.shape[0],1)))\n",
        "            else:\n",
        "              total_cost += self.calculate_cost_L2_regularised(A_last, np.reshape(Y[:,j],(Y.shape[0],1)), params, self.lambd)\n",
        "            #print('Cost : ',cost)\n",
        "            # Backward propagation.\n",
        "            if self.lambd==0:\n",
        "              grads = self.backward_propagation(A_last, np.reshape(Y[:,j],(Y.shape[0],1)), memory, self.activation_functions)\n",
        "            elif self.lambd !=0:\n",
        "              grads = self.backward_propagation_L2_regularised(A_last, np.reshape(Y[:,j],(Y.shape[0],1)), memory, self.activation_functions, self.lambd)\n",
        "         \n",
        "\n",
        "          cost = total_cost/m\n",
        "          # Update parameters.\n",
        "          params = self.gradient_descent(params, grads, self.learning_rate)\n",
        "        \n",
        "          # Print the cost every 10 iterations\n",
        "          if print_cost and i % 10 == 0 or i == self.num_iterations - 1:\n",
        "              print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
        "          if i % 10 == 0 or i == self.num_iterations:\n",
        "              costs.append(cost)\n",
        "\n",
        "      else:\n",
        "        for i in range(0, self.num_iterations):\n",
        "\n",
        "            # Forward propagation:\n",
        "            \n",
        "            A_last, memory = self.forward_propagation(X, params, self.activation_functions)\n",
        "            \n",
        "            #print('A last shape : ', A_last.shape)\n",
        "            #print('memory : ', memory.keys())\n",
        "\n",
        "            # Compute cost.\n",
        "            if self.lambd == 0:\n",
        "              cost = self.calculate_cost(A_last, Y)\n",
        "            else:\n",
        "              cost = self.calculate_cost_L2_regularised(A_last, Y, params, self.lambd)\n",
        "            #cost = self.calculate_cost(A_last, self.Y)\n",
        "                    \n",
        "            # Backward propagation.\n",
        "            if self.lambd==0:\n",
        "              grads = self.backward_propagation(A_last, Y, memory, self.activation_functions)\n",
        "            elif self.lambd !=0:\n",
        "              grads = self.backward_propagation_L2_regularised(A_last, Y, memory, self.activation_functions, self.lambd)\n",
        "          \n",
        "\n",
        "            \n",
        "            # Update parameters.\n",
        "            params = self.gradient_descent(params, grads, self.learning_rate)\n",
        "          \n",
        "            # Print the cost every 10 iterations\n",
        "            if print_cost and i % 10 == 0 or i == self.num_iterations - 1:\n",
        "                print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
        "            if i % 10 == 0 or i == self.num_iterations:\n",
        "                costs.append(cost)\n",
        "        \n",
        "      return params, costs\n",
        "\n",
        "\n",
        "  def predict(self,input_x,final_params):\n",
        "    y_pred_scores, memory_pred = self.forward_propagation(input_x, final_params, self.activation_functions)\n",
        "    y_pred = np.argmax(y_pred_scores,axis=0)\n",
        "\n",
        "    return y_pred, y_pred_scores\n",
        "\n",
        "  def accuracy(self,y_pred, y_actual):\n",
        "    acc = np.sum(np.equal(y_actual, y_pred)) / len(y_actual)\n",
        "    return acc\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okEIGJqYVKXW"
      },
      "outputs": [],
      "source": [
        "class Data():\n",
        "  def visualize(self, rows, cols):\n",
        "    plt.figure(figsize=(12,8))\n",
        "    for i in range(rows*cols):\n",
        "      plt.subplot(2,3,i+1)\n",
        "      plt.imshow(train_x_orig[i])\n",
        "\n",
        "\n",
        "  def getInfo(self, train_x_orig, train_y, test_x_orig, test_y):\n",
        "    print('X Train data size : ', train_x_orig.shape)\n",
        "    print('Y Train data size : ', train_y.shape)\n",
        "    print('X Test data size : ', test_x_orig.shape)\n",
        "    print('Y Test data size : ', test_y.shape)\n",
        "    print('='*50)\n",
        "    #print('Total training examples : ', train_y.shape[0] )\n",
        "\n",
        "\n",
        "  def preprocess_data(self,train_x_orig, train_y, test_x_orig, test_y, n_classes):\n",
        "    m_train = train_y.shape[0]\n",
        "    X_train = train_x_orig.reshape(m_train, -1).T\n",
        "    X_train = X_train/255.\n",
        "    y_train = to_categorical(train_y,n_classes)\n",
        "    y_train = y_train.T\n",
        "    \n",
        "\n",
        "    m_test = test_y.shape[0]\n",
        "    X_test = test_x_orig.reshape(m_test, -1).T\n",
        "    X_test = X_test/255.\n",
        "    y_test = to_categorical(test_y,n_classes)\n",
        "    y_test = y_test.T\n",
        "  \n",
        "\n",
        "    self.getInfo(X_train, y_train, X_test, y_test)\n",
        "    \n",
        "    return (X_train, y_train, X_test, y_test)\n",
        "    \n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RfLGZAB8b8N"
      },
      "outputs": [],
      "source": [
        "\n",
        "#without regularization\n",
        "\n",
        "layers_list = [784, 256,64, 10]\n",
        "activation_functions = ['relu','relu','relu','softmax']\n",
        "weight_initializer = 'he'\n",
        "learning_rate = 0.1\n",
        "num_iterations = 100\n",
        "lambd = 0\n",
        "drop_rate=0\n",
        "\n",
        "sgd = False\n",
        "\n",
        "my_model = My_Neural_Network(layers_list,activation_functions, weight_initializer,learning_rate,num_iterations,lambd,drop_rate, sgd)\n",
        "\n",
        "def test_suit_gradient_descent():\n",
        "\n",
        "    parameters = {'W1': np.array([[ 1.63535156, -0.62320365, -0.53718766],\n",
        "                          [-1.07799357,  0.85639907, -2.29470142]]),\n",
        "                  'b1': np.array([[ 1.74604067],\n",
        "                          [-0.75184921]]),\n",
        "                  'W2': np.array([[ 0.32171798, -0.25467393,  1.46902454],\n",
        "                          [-2.05617317, -0.31554548, -0.3756023 ],\n",
        "                          [ 1.1404819 , -1.09976462, -0.1612551 ]]),\n",
        "                  'b2': np.array([[-0.88020257],\n",
        "                          [ 0.02561572],\n",
        "                          [ 0.57539477]])}\n",
        "    grads = {'dW1': np.array([[-1.10061918,  1.14472371,  0.90159072],\n",
        "                      [ 0.50249434,  0.90085595, -0.68372786]]),\n",
        "              'db1': np.array([[-0.12289023],\n",
        "                      [-0.93576943]]),\n",
        "              'dW2': np.array([[-0.26788808,  0.53035547, -0.69166075],\n",
        "                      [-0.39675353, -0.6871727 , -0.84520564],\n",
        "                      [-0.67124613, -0.0126646 , -1.11731035]]),\n",
        "              'db2': np.array([[0.2344157 ],\n",
        "                      [1.65980218],\n",
        "                      [0.74204416]])}\n",
        "    learning_rate = 0.01\n",
        "\n",
        "    params = my_model.gradient_descent(parameters, grads, learning_rate)\n",
        "    W1 = params['W1']\n",
        "    W2 = params['W2']\n",
        "    b1 = params['b1']\n",
        "    b2 = params['b2']\n",
        "  \n",
        "    W1_0 = parameters['W1']\n",
        "    W2_0 = parameters['W2']\n",
        "    b1_0 = parameters['b1']\n",
        "    b2_0 = parameters['b2']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    assert W1.shape == W1_0.shape\n",
        "    assert b1.shape == b1_0.shape\n",
        "    assert W2.shape == W2_0.shape\n",
        "    assert b2.shape == b2_0.shape\n",
        "    \n",
        "    assert (np.round(np.sum(W1),4) == np.round(np.sum(np.array([[ 1.64635775, -0.63465089, -0.54620357],[-1.08301851,  0.84739051, -2.28786414]])),4))\n",
        "    assert (np.round(np.sum(b1),4) == np.round(np.sum(np.array([[ 1.74726957],[-0.74249152]])),4))\n",
        "    assert (np.round(np.sum(W2),4) == np.round(np.sum(np.array([[  0.32439686, -0.25997748,  1.47594115], [-2.05220563, -0.30867375, -0.36715024 ], [ 1.14719436, -1.09963797, -0.150082 ]])),4))\n",
        "    assert (np.round(np.sum(b2),4) == np.round(np.sum(np.array([[-0.88254673], [ 0.0090177], [ 0.56797433]])),4))\n",
        "\n",
        "\n",
        "\n",
        "    print(\"gradient_descent working CORRECTLY.\")\n",
        "\n",
        "  \n",
        "#test_suit_gradient_descent()\n",
        "\n",
        "def test_initialize_params():\n",
        "    layers_list = [64,32,16,10]\n",
        "    methods = ['trunc_normal','he','xavier','zeros','random']\n",
        "    W1_shape, b1_shape = (32,64), (32,1)\n",
        "    W2_shape, b2_shape = (16,32), (16,1)\n",
        "    W3_shape, b3_shape = (10,16), (10,1)\n",
        "    for method in methods:\n",
        "      params = my_model.initialize_params(layers_list, method='random')\n",
        "      W1,b1 = params['W1'], params['b1']\n",
        "      W2,b2 = params['W2'], params['b2']\n",
        "      W3,b3 = params['W3'], params['b3']\n",
        "      #print(b2.shape)\n",
        "      assert len(params) == (len(layers_list)-1)*2\n",
        "      assert W1.shape == (32,64)\n",
        "      assert b1.shape == (32,1)\n",
        "      assert W2.shape == (16,32)\n",
        "      assert b2.shape == (16,1)\n",
        "      assert W3.shape == (10,16)\n",
        "      assert b3.shape == (10,1)\n",
        "\n",
        "      print(f\"{method} weight initialzation working CORRECTLY\")\n",
        "#test_initialize_params()   \n",
        "\n",
        "def test_linear_activation():\n",
        "    W = np.array([[ 0.50288142, -1.24528809, -1.05795222]])\n",
        "    b = np.array([[-0.90900761]])\n",
        "    A = np.array([[-0.41675785, -0.05626683],\n",
        "        [-2.1361961 ,  1.64027081],\n",
        "        [-1.79343559, -0.84174737]])\n",
        "    activation_functions = ['relu','sigmoid']\n",
        "\n",
        "    (A_current, memory_current) = my_model.linear_activation(W,b,A,'sigmoid')\n",
        "    assert np.round(np.sum(A_current),4) == np.round(np.sum(np.array([[0.96890023, 0.11013289]])),4)\n",
        "    (A_current, memory_current) = my_model.linear_activation(W,b,A,'relu')\n",
        "    assert np.round(np.sum(A_current),4) == np.round(np.sum(np.array([[3.43896131, 0.        ]])),4)\n",
        "\n",
        "    print(\"linear_activation working CORRECTLY\")\n",
        "\n",
        "#test_linear_activation()\n",
        "\n",
        "\n",
        "def test_forward_propagation():\n",
        "    A0 = np.array([[-0.31178367,  0.72900392,  0.21782079, -0.8990918 ],\n",
        "        [-2.48678065,  0.91325152,  1.12706373, -1.51409323],\n",
        "        [ 1.63929108, -0.4298936 ,  2.63128056,  0.60182225],\n",
        "        [-0.33588161,  1.23773784,  0.11112817,  0.12915125],\n",
        "        [ 0.07612761, -0.15512816,  0.63422534,  0.810655  ]])\n",
        "    \n",
        "    params = {'W1': np.array([[ 0.35480861,  1.81259031, -1.3564758 , -0.46363197,  0.82465384],\n",
        "                    [-1.17643148,  1.56448966,  0.71270509, -0.1810066 ,  0.53419953],\n",
        "                    [-0.58661296, -1.48185327,  0.85724762,  0.94309899,  0.11444143],\n",
        "                    [-0.02195668, -2.12714455, -0.83440747, -0.46550831,  0.23371059]]),\n",
        "              'b1': np.array([[ 1.38503523],\n",
        "                    [-0.51962709],\n",
        "                    [-0.78015214],\n",
        "                    [ 0.95560959]]),\n",
        "              'W2': np.array([[-0.12673638, -1.36861282,  1.21848065, -0.85750144],\n",
        "                    [-0.56147088, -1.0335199 ,  0.35877096,  1.07368134],\n",
        "                    [-0.37550472,  0.39636757, -0.47144628,  2.33660781]]),\n",
        "              'b2': np.array([[ 1.50278553],\n",
        "                    [-0.59545972],\n",
        "                    [ 0.52834106]]),\n",
        "              'W3': np.array([[ 0.9398248 ,  0.42628539, -0.75815703]]),\n",
        "              'b3': np.array([[-0.16236698]])}\n",
        "             \n",
        "    activation_functions = ['relu','relu','relu','sigmoid']\n",
        "    A_last, memory = my_model.forward_propagation(A0, params, activation_functions)\n",
        "    assert np.round(np.sum(A_last),4) == np.round(np.sum(np.array([[0.03921668, 0.70498921, 0.19734387, 0.04728177]])),4)\n",
        "    print(\"forward_propagation working CORRECTLY\")\n",
        "\n",
        "\n",
        "#test_forward_propagation()\n",
        "\n",
        "def test_linear_backward():\n",
        "    dA = np.array([[-0.41675785, -0.05626683]])\n",
        "    #Z,W,b,A\n",
        "    # memory[0] = A_prev, W,b\n",
        "    # memory[1] = \n",
        "    memory = (\n",
        "              (np.array([[-2.1361961 ,  1.64027081],\n",
        "                      [-1.79343559, -0.84174737],\n",
        "                      [ 0.50288142, -1.24528809]]),\n",
        "              np.array([[-1.05795222, -0.90900761,  0.55145404]]),\n",
        "              np.array([[2.29220801]])),\n",
        "              np.array([[ 0.04153939, -1.11792545]])\n",
        "              )\n",
        "    activation_function = ['relu','relu','relu','sigmoid']\n",
        "    dA_previous, dW, db = my_model.linear_backward(dA, memory, 'sigmoid')\n",
        "    assert np.round(np.sum(dA_previous),4) == np.round(np.sum(np.array([[ 0.11017994,  0.01105339],[ 0.09466817,  0.00949723],[-0.05743092, -0.00576154]])),4)\n",
        "    assert np.round(np.sum(dW),4) == np.round(np.sum(np.array([[ 0.10266786,  0.09778551, -0.01968084]])),4)\n",
        "    assert np.round(np.sum(db),4) == np.round(np.sum(np.array([[-0.05729622]])),4)\n",
        "    dA_previous, dW, db = my_model.linear_backward(dA, memory, 'relu')\n",
        "    assert np.round(np.sum(dA_previous),4) == np.round(np.sum(np.array([[ 0.44090989,  0.        ],[ 0.37883606,  0.],[-0.2298228,   0.        ]])),4)\n",
        "    assert np.round(np.sum(dW),4) == np.round(np.sum(np.array([[ 0.44513824,  0.37371418, -0.10478989]])),4)\n",
        "    assert np.round(np.sum(db),4) == np.round(np.sum(np.array([[-0.20837892]])),4)\n",
        "    print('linear_backward working CORRECTLY')         \n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "ZhBgjy13QJEC",
        "outputId": "75094777-58a5-4177-8bee-30e8d7532f56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "X Train data size :  (784, 60000)\n",
            "Y Train data size :  (10, 60000)\n",
            "X Test data size :  (784, 10000)\n",
            "Y Test data size :  (10, 10000)\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(train_x_orig, train_y), (test_x_orig, test_y) = mnist.load_data()\n",
        "data = Data()\n",
        "data.visualize(2,3)\n",
        "X_train, y_train, X_test, y_test = data.preprocess_data(train_x_orig, train_y, test_x_orig, test_y, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79bFX26Td86T"
      },
      "source": [
        "# Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhViK2gHLIWG"
      },
      "outputs": [],
      "source": [
        "\n",
        "#without regularization\n",
        "\n",
        "X = X_train\n",
        "Y = y_train\n",
        "input_shape = X_train.shape[0]\n",
        "n_classes = len(np.unique(train_y))\n",
        "layers_list = [input_shape, 64,64, n_classes]\n",
        "activation_functions = ['relu','relu','relu','softmax']\n",
        "weight_initializer = 'he'\n",
        "learning_rate = 0.1\n",
        "num_iterations = 300\n",
        "lambd = 0\n",
        "drop_rate=0\n",
        "sgd = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "my_model = My_Neural_Network(layers_list,activation_functions, weight_initializer,learning_rate,num_iterations,lambd,drop_rate, sgd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCXQaWoS7f02",
        "outputId": "798356a8-73b3-4326-8b46-5ad229b5f2ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 2.4659753507045177\n",
            "Cost after iteration 10: 1.6757711029235762\n",
            "Cost after iteration 20: 1.1014606133801772\n",
            "Cost after iteration 30: 0.7875214948869396\n",
            "Cost after iteration 40: 0.6375185734175925\n",
            "Cost after iteration 50: 0.5533101539921559\n",
            "Cost after iteration 60: 0.4989292181898242\n",
            "Cost after iteration 70: 0.4605230007638965\n",
            "Cost after iteration 80: 0.4316528897197195\n",
            "Cost after iteration 90: 0.4090047450781276\n",
            "Cost after iteration 100: 0.39065555640229216\n",
            "Cost after iteration 110: 0.3753984988191435\n",
            "Cost after iteration 120: 0.36240822931005007\n",
            "Cost after iteration 130: 0.3511616801228981\n",
            "Cost after iteration 140: 0.34128458971647174\n",
            "Cost after iteration 150: 0.3324911060727222\n",
            "Cost after iteration 160: 0.3245799333922828\n",
            "Cost after iteration 170: 0.3173980869858314\n",
            "Cost after iteration 180: 0.31080964368683595\n",
            "Cost after iteration 190: 0.3047182004620987\n",
            "Cost after iteration 200: 0.2990584998595187\n",
            "Cost after iteration 210: 0.29379449762430593\n",
            "Cost after iteration 220: 0.2888757295897757\n",
            "Cost after iteration 230: 0.28426685475900554\n",
            "Cost after iteration 240: 0.27993162100416413\n",
            "Cost after iteration 250: 0.2758418326812062\n",
            "Cost after iteration 260: 0.2719641875646215\n",
            "Cost after iteration 270: 0.26827423570474473\n",
            "Cost after iteration 280: 0.26475963686620213\n",
            "Cost after iteration 290: 0.26140333252862535\n",
            "Cost after iteration 299: 0.2585105875796635\n"
          ]
        }
      ],
      "source": [
        "parameters, costs =  my_model.train(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "fh_MAIFFEOmT",
        "outputId": "8c000070-2c4d-4acd-a128-54c0fb06e668"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Cost')"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(1,num_iterations,10),costs)\n",
        "plt.title('Cost vs Iteration - without regularisation')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ot3tVJLcO1D",
        "outputId": "5b3c898e-b58a-4462-ab06-346bfc9774a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000,) (10000,)\n",
            "Test Accuracy :  0.9262\n",
            "(60000,) (60000,)\n",
            "Train Accuracy :  0.9264333333333333\n"
          ]
        }
      ],
      "source": [
        "y_pred, y_pred_scores = my_model.predict(X_test,parameters)\n",
        "print(y_pred.shape, test_y.shape)\n",
        "print('Test Accuracy : ',my_model.accuracy(y_pred, test_y))\n",
        "\n",
        "y_pred, y_pred_scores = my_model.predict(X_train,parameters)\n",
        "print(y_pred.shape, train_y.shape)\n",
        "print('Train Accuracy : ',my_model.accuracy(y_pred, train_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A56E2TqSFEsZ"
      },
      "source": [
        "### It will go further lower if I increase the number of epochs (iterations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mykz0AWNeFIG"
      },
      "outputs": [],
      "source": [
        "\n",
        "#without regularization\n",
        "\n",
        "X = X_train\n",
        "Y = y_train\n",
        "input_shape = X_train.shape[0]\n",
        "n_classes = len(np.unique(train_y))\n",
        "layers_list = [input_shape, 64,64,64, n_classes]\n",
        "activation_functions = ['relu','relu','relu','sigmoid','softmax']\n",
        "weight_initializer = 'he'\n",
        "learning_rate = 0.1\n",
        "num_iterations = 300\n",
        "lambd = 0\n",
        "drop_rate=0\n",
        "sgd = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "my_model2 = My_Neural_Network(layers_list,activation_functions, weight_initializer,learning_rate,num_iterations,lambd,drop_rate, sgd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbSzIUCoeQdI",
        "outputId": "8fdf848a-9341-41b3-d432-475c2c5f3c92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 2.392962488536202\n",
            "Cost after iteration 10: 2.0561691440940764\n",
            "Cost after iteration 20: 1.920683714095354\n",
            "Cost after iteration 30: 1.501379602010427\n",
            "Cost after iteration 40: 1.8838765551972527\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cost after iteration 50: 2.430533523513157\n",
            "Cost after iteration 60: 2.8781402436043613\n",
            "Cost after iteration 70: 2.0350536014360463\n",
            "Cost after iteration 80: 2.007362107396686\n",
            "Cost after iteration 90: 2.3211992872447933\n",
            "Cost after iteration 100: 2.364122265632973\n",
            "Cost after iteration 110: 2.199621788894568\n",
            "Cost after iteration 120: 2.4006707249143804\n",
            "Cost after iteration 130: 2.4130576869728437\n",
            "Cost after iteration 140: 2.328219277250035\n",
            "Cost after iteration 150: 2.267575819191284\n",
            "Cost after iteration 160: 2.206157738391534\n",
            "Cost after iteration 170: 2.3366120957407253\n",
            "Cost after iteration 180: 2.3145698241222674\n",
            "Cost after iteration 190: 2.2906126503047184\n",
            "Cost after iteration 200: 2.281255117675114\n",
            "Cost after iteration 210: 2.33671042278663\n",
            "Cost after iteration 220: 2.3020508304361913\n",
            "Cost after iteration 230: 2.2958866224737933\n",
            "Cost after iteration 240: 2.290655796693648\n",
            "Cost after iteration 250: 2.286301179352487\n",
            "Cost after iteration 260: 2.2827412212262166\n",
            "Cost after iteration 270: 2.2798746171332906\n",
            "Cost after iteration 280: 2.2775851903221915\n",
            "Cost after iteration 290: 2.2757588479217135\n",
            "Cost after iteration 299: 2.2744266169647904\n"
          ]
        }
      ],
      "source": [
        "parameters, costs =  my_model2.train(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5k3KofGgJr_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxXDHiYuVu91"
      },
      "outputs": [],
      "source": [
        "\n",
        "#without regularization\n",
        "\n",
        "X = X_train\n",
        "Y = y_train\n",
        "input_shape = X_train.shape[0]\n",
        "n_classes = len(np.unique(train_y))\n",
        "layers_list = [input_shape, 256,64, n_classes]\n",
        "activation_functions = ['relu','relu','relu','softmax']\n",
        "weight_initializer = 'he'\n",
        "learning_rate = 0.1\n",
        "num_iterations = 100\n",
        "lambd = 1000\n",
        "\n",
        "\n",
        "\n",
        "my_model_reg = My_Neural_Network(X,Y,input_shape,n_classes,layers_list,activation_functions, weight_initializer,learning_rate,num_iterations,lambd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "RPwNSrpXWAcf",
        "outputId": "b1f33c1b-0fe6-4721-e10a-f58bc9a354ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 8.013209527696\n",
            "Cost after iteration 10: 6.776575495900141\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-c6fae4b7874f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmy_model_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-eda1514c83fc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, print_cost)\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_functions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m           \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambd\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_propagation_L2_regularised\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-eda1514c83fc>\u001b[0m in \u001b[0;36mbackward_propagation_L2_regularised\u001b[0;34m(self, A_last, Y, memory, activation_functions, lambd)\u001b[0m\n\u001b[1;32m    322\u001b[0m       \u001b[0;31m#print('flkqenfnejnqn ', activation_functions[l])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m       \u001b[0mdA_previous\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dA\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0mdA_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_current\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_current\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m       \u001b[0mA_previous_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_current\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#fetching memory cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-eda1514c83fc>\u001b[0m in \u001b[0;36mlinear_backward\u001b[0;34m(self, dA, memory, activation_function)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_previous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0mdA_previous\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdA_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "parameters, costs =  my_model_reg.train()\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcLGpdQxN_lb"
      },
      "source": [
        "### Since the model was already generalizing pretty well, regularization didnt change much. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVCx1kwZra6O"
      },
      "outputs": [],
      "source": [
        "\n",
        "#without regularization\n",
        "\n",
        "X = X_train\n",
        "Y = y_train\n",
        "input_shape = X_train.shape[0]\n",
        "n_classes = len(np.unique(train_y))\n",
        "layers_list = [input_shape, 256,64, n_classes]\n",
        "activation_functions = ['relu','relu + dropout','relu + dropout','softmax']\n",
        "weight_initializer = 'he'\n",
        "learning_rate = 0.1\n",
        "num_iterations = 100\n",
        "lambd = 0\n",
        "drop_rate = 0.2\n",
        "sgd = True\n",
        "\n",
        "\n",
        "\n",
        "my_model_drop = My_Neural_Network(X,Y,input_shape,n_classes,layers_list,activation_functions, weight_initializer,learning_rate,num_iterations,lambd,drop_rate, sgd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "Xdjwjo22W8bQ",
        "outputId": "fa7bc949-e114-477c-fa9e-1e7e11d45c53"
      },
      "outputs": [
        {
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-02a6ba90db69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmy_model_drop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-9f6781e1ffe4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, print_cost)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;31m# Backward propagation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambd\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m               \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_functions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambd\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m               \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_propagation_L2_regularised\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-9f6781e1ffe4>\u001b[0m in \u001b[0;36mbackward_propagation\u001b[0;34m(self, A_last, Y, memory, activation_functions)\u001b[0m\n\u001b[1;32m    292\u001b[0m       \u001b[0;31m#print('flkqenfnejnqn ', activation_functions[l])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mdA_previous\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dA\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m       \u001b[0mdA_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_current\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_current\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m       \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dA\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdA_previous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-9f6781e1ffe4>\u001b[0m in \u001b[0;36mlinear_backward\u001b[0;34m(self, dA, memory, activation_function)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mactivation_function\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'relu + dropout'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m       \u001b[0mdA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdA\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m       \u001b[0mdA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdA\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0mdZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'D' referenced before assignment"
          ]
        }
      ],
      "source": [
        "parameters, costs =  my_model_drop.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "v1li5nSJWF0c",
        "outputId": "528bde2f-9380-4cfa-d4a3-0dd5878dd576"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-061c73c85f35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_model_drop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Accuracy : '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_model_drop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'My_Neural_Network' object has no attribute 'predict'"
          ]
        }
      ],
      "source": [
        "y_pred, y_pred_scores = my_model_drop.predict(X_test,parameters)\n",
        "print(y_pred.shape, test_y.shape)\n",
        "print('Test Accuracy : ',accuracy(y_pred, test_y))\n",
        "\n",
        "y_pred, y_pred_scores = my_model_drop.predict(X_train,parameters)\n",
        "print(y_pred.shape, train_y.shape)\n",
        "print('Train Accuracy : ',accuracy(y_pred, train_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki6_oaf5dNEV",
        "outputId": "c2310cf3-87ac-4b91-f3e2-c0b164948dc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = np.array([1,2,3,4])\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp0HvG4MPHY7",
        "outputId": "aad5669b-f4d4-4155-9ad2-7421bf7d6a0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4, 1)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z = np.reshape(y,(y.shape[0],1))\n",
        "z.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckxQLKyoPJH8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}